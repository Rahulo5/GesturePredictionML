{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture based control for consumer electronics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "X_test = []\n",
    "Y_test = []\n",
    "for i in range(25):\n",
    "    for j in range(15):\n",
    "        img = cv2.imread(\"work/u\"+ str(i+1) + \"_\" + str(j+1) + \".png\")\n",
    "        img1 = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        k = cv2.normalize(img1, dst=None, alpha=0, beta=255,norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "        blur = cv2.GaussianBlur(k,(5,5),0)\n",
    "        ret, thresh1 = cv2.threshold(blur, 60, 255, cv2.THRESH_BINARY + \n",
    "                                            cv2.THRESH_OTSU)\n",
    "        nb_components, output, stats, centroids = cv2.connectedComponentsWithStats(thresh1, connectivity=4)\n",
    "        sizes = stats[:, -1]\n",
    "\n",
    "        max_label = 1\n",
    "        max_size = sizes[1]\n",
    "        for k in range(2, nb_components):\n",
    "            if sizes[k] > max_size:\n",
    "                max_label = k\n",
    "                max_size = sizes[k]\n",
    "\n",
    "        img2 = np.zeros(output.shape)\n",
    "        img2[output == max_label] = 255\n",
    "        \n",
    "        new_image = img2[stats[max_label][1]:stats[max_label][1]+stats[max_label][3],stats[max_label][0]:stats[max_label][0]+stats[max_label][2]]\n",
    "        image_for_input = cv2.resize(img2, (100, 100), interpolation = cv2.INTER_AREA)\n",
    "        plt.imshow(image_for_input)\n",
    "        if(i<15):\n",
    "            X.append(image_for_input)\n",
    "            Y.append(j)\n",
    "        else:\n",
    "            X_test.append(image_for_input)\n",
    "            Y_test.append(j)\n",
    "        cv2.imwrite(\"work/o\"+ str(i+1) + \"_\" + str(j+1) + \".png\",thresh1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-510076b6714d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0my_binary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0my_binary_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# # Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/keras/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   2318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m     return _methods._amax(a, axis=axis,\n\u001b[0;32m-> 2320\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   2321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# small reductions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_amax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "run_opts = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "#add model layers\n",
    "\n",
    "model.add(Conv2D(100, kernel_size=3, activation=\"relu\", input_shape=(100,100,1)))\n",
    "model.add(Conv2D(100, kernel_size=3, activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(15, activation=\"softmax\"))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "y_binary = to_categorical(np.array(Y))\n",
    "y_binary_test = to_categorical(np.array(Y_test))\n",
    "\n",
    "# # Fit the model\n",
    "model.fit(np.expand_dims(np.array(X), axis=-1), y_binary, validation_data=(np.expand_dims(np.array(X_test), axis=-1), y_binary_test), epochs=5)\n",
    "\n",
    "# # evaluate the model\n",
    "scores = model.evaluate(np.expand_dims(np.array(X_test), axis=-1), y_binary_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
